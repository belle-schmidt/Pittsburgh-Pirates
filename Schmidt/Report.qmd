---
title: "Title"
author:
  - Tiger Teng
  - Liam Jennings
  - Isabelle Schmidt
date: "July 26, 2024"
toc: true
format:
  html:
    theme: cosmo
    html-math-method: katex
    self-contained: true
execute:
  echo: false
  warning: false
  message: false
---

---

## Introduction 
Over the last couple of years, Stuff+ has become increasingly popular in the game of baseball. It is a pitching metric that evaluates a pitch's effectiveness based on its physical characteristics such as velocity, vertical movement, horizontal movement, release point, and spin rate. Essentially, it is used to measure the nastiness of a pitch. The way it is measured is that a Stuff+ score of 100 represents a pitch that is considered league average for pitches thrown in that pitch-type and anything above or below 100 is considered above or below average respectively. For example, a pitcher that throws a 4-Seam Fastball with a Stuff+ of 125 would be 25% above league average for fastballs. 

With Stuff+ becoming a common metric used to evaluate pitchers, we wanted to assess how accurately the model was actually predicting a pitcher's success. We were curious if there were any variables that the model was over or under predicting in terms of their importance in a pitcher's success. We also wanted to know if there were certain times the model tended to over or under predict success, telling us it was not accounting for variables that played a role in a pitcher's effectiveness. 

By answering these questions, we hope to provide valuable information to help build a better Stuff+ model so that it becomes a more reliable metric for people involved in player acquisition. We also believe that getting a better understanding of which physical pitch characteristics play a larger role in success can help pitchers develop their own pitches in a way that allows them to be more successful. 

## Data
For our analysis, we used FanGraphs and Statcast data from 2020 to 2024. We downloaded the FanGraphs data from ______ and the Statcast data from Baseball Savant. The FanGraphs data provides each pitcher's Stuff+ for each pitch they throw in each season. Note that unless specified otherwise, when we say pitch throughout this paper, we are referring to pitch name and not an individual pitch. This data was originally provided to us so that each row provided information about a pitcher in a given year. However, we used pivot longer to make each row represent a pitcher in a given year for a specific pitch. For example, instead of one row having information about all of Paul Skene's pitches from the 2024 season, it became that one row had information about Paul Skene's sinker in the 2024 season and another had information about his 4-Seam Fastball in the 2024 season. Statcast's data, on the other hand, is pitch-by-pitch data that gives information about all the individual pitches thrown. Since FanGraphs only has Stuff+ data from 2020 to 2024, we only included Statcast data from 2020 until ____ in the 2024 season. 

From the Statcast data we focused primarily on the pitch physical characteristics used to measure Stuff+, in addition to, run value, expected weighted on-base average (xwOBA), and outcome description. For each pitch physical characteristic individually, xwOBA, and run value per 100 pitches, we grouped the data by season, pitcher, and pitch name to calculate each pitcher's average for each pitch in each season. Using the same grouping and the outcome description, we also calculated each pitcher's whiff percentage, swing and miss percentage, and swing percentage for each pitch in each season. We also calculated the number of times each pitcher threw each pitch in each season. Once all these values were calculated, we combined the FanGraphs Dataset to the Statcast Dataset so that we could evaluate all these metrics simultaneously. It is also important to note that in our actual analysis, we only included pitchers that threw that given pitch 100 or more times to limit bias in our results. 

In our initial analysis, we decided to focus on the 4-Seam Fastball, often known as the rising fastball. In all of the seasons we analyzed, it was by far the most commonly used pitch. In addition... ** Ask Liam about Dodgers bringing it back in 2017?** 

From there, the first thing we wanted to find out was whether or not the Stuff+ model changed from year-to-year. To figure this out, we made a handful of scatter plots with smooth lines as shown in Figure 1. Each graph had a pitcher's average of a single pitch physical characteristic on the x-axis and their average Stuff+ on the y-axis. The plots were also all faceted by year so that we could see how the relationship between the given pitch physical characteristic and Stuff+ has changed each year. After analyzing the graphs, it became clear that the Stuff+ model was changing at least a little bit each year.  

* In particular, for how it accounted for velocity and horizontal break. Induced vertical break was a little more consistent but still saw some variation.* - Don't know if I want to say this because it really changed for all but that was just what we focused on 

Seeing that the Stuff+ model was in fact changing each year made us wonder how well the model was adjusting, assuming that it needed to be adjusted at all. To first get a basic understanding of whether or not outcomes were also changing based on the pitch physical characteristics each year, we made the same plots mentioned above but with an outcome variable on the y-axis instead of Stuff+. Outcome variables being whiff percentage, swing and miss percentage, swing percentage, expected weighted on-base average, and run value per 100 pitches. Figure 2 is an example where ____ is on x-axis and ____ is on the y-axis. Our group decided to focus primarily on whiff percentage and expected weighted on-base average. We liked that whiff percentage gave us an idea of whether people were making contact with the pitch or not while expected weighted on-base average gave us an idea of (a) whether or not people were able to get on base and (b) how well people were making contact with the pitch. We found that the impact each additional unit of a pitch physical characteristic had on an outcome variable was indeed changing each year, indicating that it was good that the Stuff+ model was changing. 

To make it easier to evaluate whether or not the Stuff+ model was changing effectively, we made linear models that better represented how each additional unit of a physical characteristic impacted Stuff+, whiff percentage, and expected on-base average. 

** Talk to Quang or Ron about linear models! 




Steps we want to mention:
- Focused on 4-Seam Fastball First
- Looked at how the stuff+ model changed for each characteristic (told us that the model was changing year by year)
- Looked at how each characteristic changed the outcome, saw some made more impact than others 
- Found it was hard to eyeball so created the slope plots to quantify the difference
- Liam did his analysis on the hex plot to see how some of the physical characteristics worked together. 

Goes into methods: 
- From these graphs we only continued to see that there were some characteristics that appeared to be undervalued and others that appeared to be overvalued. We thought of xwOBA and whiff percentage the most important for evaluating a pitcher's success so we compared how the stuff+ model valued those in each year compared to how much it mattered for whiff percentage and xwOBA in each year. 

_ If have time then say we wanted to see if similar trends were happening with slider/ sweepers since those were becoming more popular. 


- Since sweepers are becoming increasingly popular we also wanted to look at those. 

EDA we want to include/ mention:
- The relationship plots 
- The slope plots 
- Liam's hex plots 
- If we include sweeper then show how that pitch is more popular



Notes on data part:
2. Note if the data was lacking anything 
3. Maybe give an example
4. Give information about how many observations were in the data and how many we took out
5. Mention what filters we put on, so only looked at pitchers that threw 100+ pitches of the given pitch 


Describe the data you’re using in detail, where you accessed it, along with relevant exploratory data analysis (EDA). You should also include descriptions of any major data pre-processing/cleaning steps.

## Methods

Describe the modeling techniques you chose, their assumptions, justifications for why they are appropriate for the problem, and your plan for comparison/evaluation approaches.

## Results

Describe your results. This can include tables and plots showing your results, as well as text describing how your models worked and the appropriate interpretations of the relevant output. (Note: Don’t just write out the textbook interpretations of all model coefficients! Instead, interpret the output that is relevant for your question of interest that is framed in the introduction)

## Discussion

Give your conclusions and summarize what you have learned with regards to your question of interest. Are there any limitations with the approaches you used? What do you think are the next steps to follow-up your project?

## Appendix: A quick tutorial

**(Feel free to remove this section when you submit)**

This a Quarto document. 
To learn more about Quarto see <https://quarto.org>.
You can use the Render button to see what it looks like in HTML.

### Text formatting

Text can be bolded with **double asterisks** and italicized with *single asterisks*. 
Monospace text, such as for short code snippets, uses `backticks`.
(Note these are different from quotation marks or apostrophes.) Links are
written [like this](http://example.com/).

Bulleted lists can be written with asterisks:

* Each item starts on a new line with an asterisk.
* Items should start on the beginning of the line.
* Leave blank lines after the end of the list so the list does not continue.

Mathematics can be written with LaTeX syntax using dollar signs. 
For instance, using single dollar signs we can write inline math: $(-b \pm \sqrt{b^2 - 4ac})/2a$.

To write math in "display style", i.e. displayed on its own line centered on the
page, we use double dollar signs:
$$
x^2 + y^2 = 1
$$


### Code blocks

Code blocks are evaluated sequentially when you hit Render. 
As the code runs, `R` prints out which block is running, so naming blocks is useful if you want to know which one takes a long time. 
After the block name, you can specify [chunk options](https://yihui.org/knitr/options/). 
For example, `echo` controls whether the code is printed in the document. 
By default, output is printed in the document in monospace:

```{r, echo = FALSE}
head(mtcars)
```

Chunk options can also be written inside the code block, which is helpful for really long options, as we'll see soon.

```{r}
#| echo: false
head(mtcars)
```

### Figures

If a code block produces a plot or figure, this figure will automatically be inserted inline in the report. That is, it will be inserted exactly where the code block is.

```{r}
#| fig-width: 5
#| fig-height: 3.5
#| fig-cap: "This is a caption. It should explain what's in the figure and what's interesting about it. For instance: There is a negative, strong linear correlation between miles per gallon and horsepower for US cars in the 1970s."

library(tidyverse)
mtcars |> 
  ggplot(aes(x = mpg, y = hp)) +
  geom_point() +
  labs(x = "Miles per gallon",
       y = "Horsepower")
```

Notice the use of `fig-width` and `fig-height` to control the figure's size (in inches). 
These control the sizes given to `R` when it generates the plot, so `R` proportionally adjusts the font sizes to be large enough.

### Tables

Use the `knitr::kable()` function to print tables as HTML:

```{r}
mtcars |> 
  slice(1:5) |> 
  knitr::kable()
```

We can summarize model results with a table. 
For instance, suppose we fit a linear regression model:

```{r}
#| echo: true
model1 <- lm(mpg ~ disp + hp + drat, data = mtcars)
```

It is *not* appropriate to simply print `summary(model1)` into the report. 
If we want the reader to understand what models we have fit and what their results are, we should provide a nicely formatted table. 
A simple option is to use the `tidy()` function from the `broom` package to get a data frame of the model fit, and simply report that as a table.

```{r }
#| results: "asis"
#| tbl-cap: "Predicting fuel economy using vehicle features."

library(broom)
model1 |> 
  tidy() |>
  knitr::kable(digits = 2,
               col.names = c("Term", "Estimate", "SE", "t", "p"))
```